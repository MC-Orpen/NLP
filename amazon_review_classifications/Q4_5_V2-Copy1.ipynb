{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv                               # csv reader\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.classify import SklearnClassifier\n",
    "from random import shuffle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating the loadData and splitData functions\n",
    "The loadData and splitData functions were updated so the verified purchase, rating and category of each review were included as features within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from a file and append it to the rawData\n",
    "def loadData(path, Text=None):\n",
    "    with open(path) as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        for line in reader:\n",
    "            if line[0] == \"DOC_ID\":  # skip the header\n",
    "                continue\n",
    "            (Id, Text, Label, Verified, Rating, Category) = parseReview(line)\n",
    "            rawData.append((Id, Text, Label, Verified, Rating, Category))\n",
    "\n",
    "def splitData(percentage):\n",
    "    # A method to split the data between trainData and testData \n",
    "    dataSamples = len(rawData)\n",
    "    halfOfData = int(len(rawData)/2)\n",
    "    trainingSamples = int((percentage*dataSamples)/2)\n",
    "    for (_, Text, Label, Verified, Rating, Category) in rawData[:trainingSamples] + rawData[halfOfData:halfOfData+trainingSamples]:\n",
    "        trainData.append((toFeatureVector(preProcess(Text), Verified, Rating, Category),Label))\n",
    "    for (_, Text, Label, Verified, Rating, Category) in rawData[trainingSamples:halfOfData] + rawData[halfOfData+trainingSamples:]:\n",
    "        testData.append((toFeatureVector(preProcess(Text), Verified, Rating, Category),Label))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "The parseReview function was also updated to extract the required information from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert line from input file into an id/text/label tuple\n",
    "def parseReview(reviewLine):\n",
    "    # Should return a triple of an integer, a string containing the review, and a string indicating the label\n",
    "    \"\"\"The ID, text and label are found by slicing each line. The ID is \n",
    "    the first element and the label is the second element. The text is\n",
    "    the last element\"\"\"\n",
    "    \n",
    "    doc_id = reviewLine[0]\n",
    "    review_text = reviewLine[-1]\n",
    "    label = reviewLine[1]\n",
    "    verified = reviewLine[3]\n",
    "    rating = reviewLine[2]\n",
    "    category = reviewLine[4]\n",
    "    if label == \"__label1__\":\n",
    "        label = \"fake\"\n",
    "    else:\n",
    "        label= \"real\"\n",
    "    \n",
    "    \n",
    "    return (doc_id, review_text, label, verified, rating, category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEXT PREPROCESSING AND FEATURE VECTORIZATION\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# Input: a string of one review\n",
    "def preProcess(text):\n",
    "    # Should return a list of tokens\n",
    "    # DESCRIBE YOUR METHOD IN WORDS\n",
    "    \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    token_norm = [token.lower() for token in tokens]\n",
    "    \n",
    "    punct = WordPunctTokenizer()\n",
    "    punc_tokens = punct.tokenize(text)\n",
    "    punc_norm = [token.lower() for token in punc_tokens]\n",
    "    \n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "#     filtered = [word for word in tokens if word not in stop_words]\n",
    "#     filtered_norm = [word for word in token_norm if word not in stop_words]\n",
    "\n",
    "#     filtered_punc = [word for word in punc_tokens if word not in stop_words]\n",
    "#     filtered_punc_norm = [word for word in punc_norm if word not in stop_words]\n",
    "    \n",
    "#     filtered_alnum = [word for word in filtered if word.isalnum()]\n",
    "#     filtered_punc_alnum = [word for word in filtered_punc if word.isalnum()]\n",
    "#     filtered_punc_norm_alnum = [word for word in filtered_punc_norm if word.isalnum()]\n",
    "\n",
    "#     bigrams = list(ngrams(token_norm, 2))\n",
    "#     bigram_punc = list(ngrams(punc_norm, 2))\n",
    "    \n",
    "#     lmtzr = WordNetLemmatizer()\n",
    "#     lm_tokens = [lmtzr.lemmatize(token) for token in tokens]\n",
    "#     lm_norm = [lmtzr.lemmatize(token) for token in token_norm]\n",
    "#     lm_punct = [lmtzr.lemmatize(token) for token in punc_tokens]\n",
    "#     lm_punct_norm = [lmtzr.lemmatize(token) for token in punc_tokens]\n",
    "    \n",
    "#     lm_filtered_tokens = [lmtzr.lemmatize(token) for token in filtered]\n",
    "#     lm_filtered_norm_tokens = [lmtzr.lemmatize(token) for token in filtered_norm]\n",
    "#     lm_f_big = list(ngrams(lm_filtered_norm_tokens, 2))\n",
    "    return punc_norm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "The new features did not require preprocessing and underwent feature vectorisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureDict = {} # A global dictionary of features\n",
    "\n",
    "def toFeatureVector(tokens, verified, rating, category):\n",
    "\n",
    "    localDict = {}\n",
    "    \n",
    "    for token in tokens:\n",
    "        try:\n",
    "            i = featureDict[token]\n",
    "        except KeyError:\n",
    "            i = len(featureDict) + 1\n",
    "            featureDict[token] = i\n",
    "        try:\n",
    "            localDict[i] += (1.0/len(tokens))\n",
    "        except KeyError:\n",
    "            localDict[i] = (1.0/len(tokens))\n",
    "\n",
    "# The purchase verification data was either 'Y' or 'N'. The features were added were set so \n",
    "# a verfied purchase had a value of 1, whilst a non-verified instance had a value of 0.\n",
    "\n",
    "    if verified == 'Y':\n",
    "        try:\n",
    "            j = featureDict['VP']\n",
    "        except KeyError:\n",
    "            j = len(featureDict) +1\n",
    "            featureDict['VP'] = 0\n",
    "        localDict[j] = 0\n",
    "    if verified == 'N':\n",
    "        try:\n",
    "            j = featureDict['VP']\n",
    "        except KeyError:\n",
    "            j = len(featureDict) +1\n",
    "            featureDict['VP'] = j\n",
    "        localDict[j] = 1.1\n",
    "\n",
    "    # The rating was then included in the features\n",
    "    try:\n",
    "        k = featureDict['RATING']\n",
    "    except KeyError:\n",
    "        k = len(featureDict) +1\n",
    "        featureDict['RATING'] = k\n",
    "    localDict[k] = int(rating)\n",
    "\n",
    "#     The last feature added was the category of the item.\n",
    "    try:\n",
    "        l = featureDict[category]\n",
    "    except KeyError:\n",
    "        l = len(featureDict) +1\n",
    "        featureDict[category] = l\n",
    "    localDict[l] = 1\n",
    "\n",
    "    return localDict\n",
    "# toFeatureVector(preProcess(dog[1]), dog[3], dog[4], dog[5])\n",
    "# featureDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING AND VALIDATING OUR CLASSIFIER\n",
    "def trainClassifier(trainData):\n",
    "    print(\"Training Classifier...\")\n",
    "    pipeline =  Pipeline([('svc', LinearSVC(dual=True, max_iter=20000))])\n",
    "    return SklearnClassifier(pipeline).train(trainData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def crossValidate(dataset, folds):\n",
    "    \"\"\"Cross validate splits the dataset into a number of folds, which \n",
    "    resamples the data to evaluate the model of the classifier. The data is \n",
    "    split into training and testing data by a number of folds, which then validate\n",
    "    the model and assess different characteristics. The crossValidate function\n",
    "    gives the averaged precision, recall, f-score and accuracy of the model.\"\"\"\n",
    "\n",
    "    # The data is first shuffled\n",
    "    shuffle(dataset)\n",
    "    \n",
    "    # Initial values of each characteristic are set to 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    f_score = 0\n",
    "    accuracy = 0\n",
    "    k_num = 1\n",
    "    \n",
    "    # The size of the test folds\n",
    "    foldSize = int(len(dataset)/folds)\n",
    "    \n",
    "    # The loop of the cross-validate.\n",
    "    # The dataset is split into testing and training data sets\n",
    "    \n",
    "    for i in range(0,len(dataset),foldSize):\n",
    "        \"\"\"Splits the data into testing and training sets\"\"\"\n",
    "        print(f'K-fold number: {k_num}')\n",
    "        k_num +=1\n",
    "        \n",
    "        testing_data = dataset[i:i+foldSize]\n",
    "        print(f'Testing data length: {len(testing_data)}')\n",
    "        training_data = dataset[0:i] + dataset[(i+foldSize):]\n",
    "        print(f'Training data length: {len(training_data)}')\n",
    "        \n",
    "        # Create classifier using each training set \n",
    "        # The true labels for the corresponding testing data are found\n",
    "        classifier = trainClassifier(training_data)\n",
    "        true_labels = [t[1] for t in testing_data]\n",
    "\n",
    "        # Prediction on the unseen test data using the classifer\n",
    "        test_pred = predictLabels(testing_data, classifier)\n",
    "        \n",
    "        # The precision, recall and f-score of the results\n",
    "        (p, r, f, s) = metrics.precision_recall_fscore_support(true_labels, test_pred, average='weighted')\n",
    "        \n",
    "        # The values found in each loop are totalled\n",
    "        precision += p\n",
    "        recall += r\n",
    "        f_score += f\n",
    "        \n",
    "        # The accuracy of the classifer is found and summed\n",
    "        a = metrics.accuracy_score(true_labels, test_pred)\n",
    "        accuracy += a\n",
    "        continue # Replace by code that trains and tests on the 10 folds of data in the dataset\n",
    "    \n",
    "    # The mean of the characteristics is found and returned.\n",
    "    precision /= folds\n",
    "    recall /= folds \n",
    "    f_score /= folds\n",
    "    accuracy /= folds\n",
    "    cv_results = [precision, recall, f_score, accuracy]\n",
    "    \n",
    "    return cv_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTING LABELS GIVEN A CLASSIFIER\n",
    "# Use predict labels\n",
    "def predictLabels(reviewSamples, classifier):\n",
    "    return classifier.classify_many(map(lambda t: t[0], reviewSamples))\n",
    "\n",
    "def predictLabel(reviewSample, classifier):\n",
    "    return classifier.classify(toFeatureVector(preProcess(reviewSample)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now 0 rawData, 0 trainData, 0 testData\n",
      "Preparing the dataset...\n",
      "Now 21000 rawData, 0 trainData, 0 testData\n",
      "Preparing training and test data...\n",
      "After split, 21000 rawData, 16800 trainData, 4200 testData\n",
      "Training Samples: \n",
      "16800\n",
      "Features: \n",
      "35670\n"
     ]
    }
   ],
   "source": [
    "# MAIN\n",
    "\n",
    "# loading reviews\n",
    "# initialize global lists that will be appended to by the methods below\n",
    "rawData = []          # the filtered data from the dataset file (should be 21000 samples)\n",
    "trainData = []        # the pre-processed training data as a percentage of the total dataset (currently 80%, or 16800 samples)\n",
    "testData = []         # the pre-processed test data as a percentage of the total dataset (currently 20%, or 4200 samples)\n",
    "\n",
    "# the output classes\n",
    "fakeLabel = 'fake'\n",
    "realLabel = 'real'\n",
    "\n",
    "# references to the data files\n",
    "reviewPath = 'amazon_reviews.txt'\n",
    "\n",
    "# Do the actual stuff (i.e. call the functions we've made)\n",
    "# We parse the dataset and put it in a raw data list\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Preparing the dataset...\",sep='\\n')\n",
    "loadData(reviewPath) \n",
    "\n",
    "# We split the raw dataset into a set of training data and a set of test data (80/20)\n",
    "# You do the cross validation on the 80% (training data)\n",
    "# We print the number of training samples and the number of features before the split\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Preparing training and test data...\",sep='\\n')\n",
    "splitData(0.8)\n",
    "# We print the number of training samples and the number of features after the split\n",
    "print(\"After split, %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Training Samples: \", len(trainData), \"Features: \", len(featureDict), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'when': 1,\n",
       " 'least': 2,\n",
       " 'you': 3,\n",
       " 'think': 4,\n",
       " 'so': 5,\n",
       " ',': 6,\n",
       " 'this': 7,\n",
       " 'product': 8,\n",
       " 'will': 9,\n",
       " 'save': 10,\n",
       " 'the': 11,\n",
       " 'day': 12,\n",
       " '.': 13,\n",
       " 'just': 14,\n",
       " 'keep': 15,\n",
       " 'it': 16,\n",
       " 'around': 17,\n",
       " 'in': 18,\n",
       " 'case': 19,\n",
       " 'need': 20,\n",
       " 'for': 21,\n",
       " 'something': 22,\n",
       " 'VP': 23,\n",
       " 'RATING': 24,\n",
       " 'PC': 25,\n",
       " 'lithium': 26,\n",
       " 'batteries': 27,\n",
       " 'are': 28,\n",
       " 'new': 29,\n",
       " 'introduced': 30,\n",
       " 'market': 31,\n",
       " 'there': 32,\n",
       " 'average': 33,\n",
       " 'developing': 34,\n",
       " 'cost': 35,\n",
       " 'is': 36,\n",
       " 'relatively': 37,\n",
       " 'high': 38,\n",
       " 'but': 39,\n",
       " 'stallion': 40,\n",
       " 'doesn': 41,\n",
       " \"'\": 42,\n",
       " 't': 43,\n",
       " 'compromise': 44,\n",
       " 'on': 45,\n",
       " 'quality': 46,\n",
       " 'and': 47,\n",
       " 'provides': 48,\n",
       " 'us': 49,\n",
       " 'with': 50,\n",
       " 'best': 51,\n",
       " 'at': 52,\n",
       " 'a': 53,\n",
       " 'low': 54,\n",
       " '.<': 55,\n",
       " 'br': 56,\n",
       " '/>': 57,\n",
       " 'many': 58,\n",
       " 'built': 59,\n",
       " 'technical': 60,\n",
       " 'assistants': 61,\n",
       " 'that': 62,\n",
       " 'act': 63,\n",
       " 'like': 64,\n",
       " 'sensor': 65,\n",
       " 'their': 66,\n",
       " 'particular': 67,\n",
       " 'forté': 68,\n",
       " 'battery': 69,\n",
       " 'keeps': 70,\n",
       " 'my': 71,\n",
       " 'phone': 72,\n",
       " 'charged': 73,\n",
       " 'up': 74,\n",
       " 'works': 75,\n",
       " 'every': 76,\n",
       " 'voltage': 77,\n",
       " 'never': 78,\n",
       " 'risked': 79,\n",
       " 'Wireless': 80,\n",
       " 'i': 81,\n",
       " 'purchased': 82,\n",
       " 'swing': 83,\n",
       " 'baby': 84,\n",
       " 'she': 85,\n",
       " '6': 86,\n",
       " 'months': 87,\n",
       " 'now': 88,\n",
       " 'has': 89,\n",
       " 'pretty': 90,\n",
       " 'much': 91,\n",
       " 'out': 92,\n",
       " 'grown': 93,\n",
       " 'very': 94,\n",
       " 'loud': 95,\n",
       " 'well': 96,\n",
       " 'beautiful': 97,\n",
       " 'though': 98,\n",
       " 'love': 99,\n",
       " 'colors': 100,\n",
       " 'lot': 101,\n",
       " 'of': 102,\n",
       " 'settings': 103,\n",
       " 'don': 104,\n",
       " 'was': 105,\n",
       " 'worth': 106,\n",
       " 'money': 107,\n",
       " 'Baby': 108,\n",
       " 'looking': 109,\n",
       " 'an': 110,\n",
       " 'inexpensive': 111,\n",
       " 'desk': 112,\n",
       " 'calcolatur': 113,\n",
       " 'here': 114,\n",
       " 'does': 115,\n",
       " 'everything': 116,\n",
       " 'only': 117,\n",
       " 'issue': 118,\n",
       " 'tilts': 119,\n",
       " 'slightly': 120,\n",
       " 'to': 121,\n",
       " 'one': 122,\n",
       " 'side': 123,\n",
       " 'hit': 124,\n",
       " 'any': 125,\n",
       " 'keys': 126,\n",
       " 'rocks': 127,\n",
       " 'little': 128,\n",
       " 'bit': 129,\n",
       " 'not': 130,\n",
       " 'big': 131,\n",
       " 'deal': 132,\n",
       " 'Office Products': 133,\n",
       " 'use': 134,\n",
       " 'twice': 135,\n",
       " 'week': 136,\n",
       " 'results': 137,\n",
       " 'great': 138,\n",
       " 'have': 139,\n",
       " 'used': 140,\n",
       " 'other': 141,\n",
       " 'teeth': 142,\n",
       " 'whitening': 143,\n",
       " 'solutions': 144,\n",
       " 'most': 145,\n",
       " 'them': 146,\n",
       " 'same': 147,\n",
       " 'would': 148,\n",
       " 'three': 149,\n",
       " 'times': 150,\n",
       " 'using': 151,\n",
       " 'because': 152,\n",
       " 'potency': 153,\n",
       " 'solution': 154,\n",
       " 'also': 155,\n",
       " 'technique': 156,\n",
       " 'trays': 157,\n",
       " 'mouth': 158,\n",
       " 'Beauty': 159,\n",
       " 'm': 160,\n",
       " 'sure': 161,\n",
       " 'what': 162,\n",
       " 'supposed': 163,\n",
       " 'be': 164,\n",
       " 'recommend': 165,\n",
       " 'do': 166,\n",
       " 'more': 167,\n",
       " 'research': 168,\n",
       " 'into': 169,\n",
       " 'culture': 170,\n",
       " 'pipes': 171,\n",
       " 'if': 172,\n",
       " 'plan': 173,\n",
       " 'giving': 174,\n",
       " 'as': 175,\n",
       " 'gift': 176,\n",
       " 'or': 177,\n",
       " 'yourself': 178,\n",
       " 'Health & Personal Care': 179,\n",
       " 'pleased': 180,\n",
       " 'ping': 181,\n",
       " 'pong': 182,\n",
       " 'table': 183,\n",
       " '11': 184,\n",
       " 'year': 185,\n",
       " 'old': 186,\n",
       " '13': 187,\n",
       " 'having': 188,\n",
       " 'blast': 189,\n",
       " 'plus': 190,\n",
       " 'lots': 191,\n",
       " 'family': 192,\n",
       " 'entertainment': 193,\n",
       " 'too': 194,\n",
       " 'better': 195,\n",
       " 'than': 196,\n",
       " 'kids': 197,\n",
       " 'sitting': 198,\n",
       " 'video': 199,\n",
       " 'games': 200,\n",
       " 'all': 201,\n",
       " 'friend': 202,\n",
       " 'put': 203,\n",
       " 'together': 204,\n",
       " 'believe': 205,\n",
       " 'challenge': 206,\n",
       " 'nothing': 207,\n",
       " 'they': 208,\n",
       " 'could': 209,\n",
       " 'handle': 210,\n",
       " 'Toys': 211,\n",
       " 'vitamin': 212,\n",
       " 'c': 213,\n",
       " 'serum': 214,\n",
       " '...': 215,\n",
       " 'really': 216,\n",
       " 'oil': 217,\n",
       " 'feeling': 218,\n",
       " 'sticky': 219,\n",
       " 'last': 220,\n",
       " 'some': 221,\n",
       " 'recent': 222,\n",
       " 'bug': 223,\n",
       " 'bites': 224,\n",
       " 'helps': 225,\n",
       " 'heal': 226,\n",
       " 'skin': 227,\n",
       " 'faster': 228,\n",
       " 'normal': 229,\n",
       " 've': 230,\n",
       " 'tide': 231,\n",
       " 'pods': 232,\n",
       " 'laundry': 233,\n",
       " 'detergent': 234,\n",
       " 'years': 235,\n",
       " 'its': 236,\n",
       " 'such': 237,\n",
       " 'nice': 238,\n",
       " 'scent': 239,\n",
       " 'leaver': 240,\n",
       " 'cloths': 241,\n",
       " 'smelling': 242,\n",
       " 'fresh': 243,\n",
       " 'everybody': 244,\n",
       " 'wants': 245,\n",
       " 'fall': 246,\n",
       " 'promises': 247,\n",
       " 'unheard': 248,\n",
       " 'brand': 249,\n",
       " 'even': 250,\n",
       " 'say': 251,\n",
       " 'non': 252,\n",
       " 'existant': 253,\n",
       " 'company': 254,\n",
       " 'look': 255,\n",
       " 'how': 256,\n",
       " 'amateur': 257,\n",
       " 'labels': 258,\n",
       " 'products': 259,\n",
       " 'ask': 260,\n",
       " 'trust': 261,\n",
       " 'kind': 262,\n",
       " 'stuff': 263,\n",
       " '?': 264,\n",
       " 'no': 265,\n",
       " 'way': 266,\n",
       " '!': 267,\n",
       " 'waste': 268,\n",
       " 'your': 269,\n",
       " 'unfortunately': 270,\n",
       " 'didn': 271,\n",
       " 'work': 272,\n",
       " 'me': 273,\n",
       " 'made': 274,\n",
       " 'sick': 275,\n",
       " 'throw': 276,\n",
       " 'once': 277,\n",
       " 'two': 278,\n",
       " 'tried': 279,\n",
       " 'kettle': 280,\n",
       " 'daughter': 281,\n",
       " 'said': 282,\n",
       " 'easy': 283,\n",
       " 'likes': 284,\n",
       " 'design': 285,\n",
       " 'neat': 286,\n",
       " 'Kitchen': 287,\n",
       " 'tumbler': 288,\n",
       " 'drinks': 289,\n",
       " 'warm': 290,\n",
       " 'cold': 291,\n",
       " 'good': 292,\n",
       " 'period': 293,\n",
       " 'time': 294,\n",
       " 'sturdy': 295,\n",
       " 'get': 296,\n",
       " 'complements': 297,\n",
       " 'sleek': 298,\n",
       " 'looks': 299,\n",
       " 'especially': 300,\n",
       " 'doesnt': 301,\n",
       " 'leave': 302,\n",
       " 'fingerprints': 303,\n",
       " 'highly': 304,\n",
       " 'suggestion': 305,\n",
       " 'washing': 306,\n",
       " 'lid': 307,\n",
       " 'shake': 308,\n",
       " 'dry': 309,\n",
       " 'prevent': 310,\n",
       " 'dripping': 311,\n",
       " '3': 312,\n",
       " 'stars': 313,\n",
       " 'cheap': 314,\n",
       " 'mild': 315,\n",
       " 'sedative': 316,\n",
       " 'effect': 317,\n",
       " 's': 318,\n",
       " 'useless': 319,\n",
       " 'stress': 320,\n",
       " '/': 321,\n",
       " 'anxiety': 322,\n",
       " 'panic': 323,\n",
       " 'attacks': 324,\n",
       " 'ad': 325,\n",
       " 'zenrx': 326,\n",
       " 'night': 327,\n",
       " 'difference': 328,\n",
       " '.[[': 329,\n",
       " 'asin': 330,\n",
       " ':': 331,\n",
       " 'b00hbgbry4': 332,\n",
       " '-': 333,\n",
       " 'dietary': 334,\n",
       " 'supplement': 335,\n",
       " 'reduces': 336,\n",
       " 'symptoms': 337,\n",
       " 'depression': 338,\n",
       " 'boost': 339,\n",
       " 'mood': 340,\n",
       " 'increase': 341,\n",
       " 'relaxation': 342,\n",
       " 'beat': 343,\n",
       " 'kava': 344,\n",
       " '5': 345,\n",
       " 'htp': 346,\n",
       " 'theanine': 347,\n",
       " 'gaba': 348,\n",
       " 'back': 349,\n",
       " 'guarantee': 350,\n",
       " '!]]': 351,\n",
       " 'bought': 352,\n",
       " '2': 353,\n",
       " 'these': 354,\n",
       " 'brown': 355,\n",
       " 'island': 356,\n",
       " 'hydraulic': 357,\n",
       " 'mechanism': 358,\n",
       " 'might': 359,\n",
       " 'fail': 360,\n",
       " 'over': 361,\n",
       " 'otherwise': 362,\n",
       " 'color': 363,\n",
       " 'accurate': 364,\n",
       " 'description': 365,\n",
       " 'padding': 366,\n",
       " 'definitely': 367,\n",
       " 'chair': 368,\n",
       " 'lounge': 369,\n",
       " 'bar': 370,\n",
       " 'counter': 371,\n",
       " 'kitchen': 372,\n",
       " 'Furniture': 373,\n",
       " 'wonderful': 374,\n",
       " 'headphones': 375,\n",
       " 'had': 376,\n",
       " 'price': 377,\n",
       " 'sound': 378,\n",
       " 'clear': 379,\n",
       " 'soft': 380,\n",
       " 'ears': 381,\n",
       " 'mention': 382,\n",
       " 'sony': 383,\n",
       " 'Electronics': 384,\n",
       " 'superb': 385,\n",
       " 'fits': 386,\n",
       " 'fine': 387,\n",
       " 'real': 388,\n",
       " 'mirror': 389,\n",
       " '(': 390,\n",
       " 'enhanced': 391,\n",
       " '),': 392,\n",
       " 'gripe': 393,\n",
       " 'wish': 394,\n",
       " 'cameras': 395,\n",
       " 'by': 396,\n",
       " 'due': 397,\n",
       " 'where': 398,\n",
       " 'left': 399,\n",
       " 'camera': 400,\n",
       " 'place': 401,\n",
       " 'wasted': 402,\n",
       " 'recording': 403,\n",
       " 'room': 404,\n",
       " 'start': 405,\n",
       " 'turn': 406,\n",
       " 'towards': 407,\n",
       " 'driver': 408,\n",
       " 'happy': 409,\n",
       " 'far': 410,\n",
       " 'Camera': 411,\n",
       " 'sling': 412,\n",
       " 'shots': 413,\n",
       " 'go': 414,\n",
       " 'enough': 415,\n",
       " 'purposes': 416,\n",
       " 'preferred': 417,\n",
       " 'slimmer': 418,\n",
       " 'grip': 419,\n",
       " 'Sports': 420,\n",
       " 'fair': 421,\n",
       " 'slicer': 422,\n",
       " 'idea': 423,\n",
       " 'tricks': 424,\n",
       " 'spiral': 425,\n",
       " 'mean': 426,\n",
       " 'manual': 427,\n",
       " 'been': 428,\n",
       " 'helpful': 429,\n",
       " '/><': 430,\n",
       " 'saying': 431,\n",
       " 'negative': 432,\n",
       " 'about': 433,\n",
       " 'however': 434,\n",
       " 'did': 435,\n",
       " 'took': 436,\n",
       " 'almost': 437,\n",
       " 'trying': 438,\n",
       " 'learn': 439,\n",
       " 'cutting': 440,\n",
       " 'different': 441,\n",
       " 'kinds': 442,\n",
       " 'vegetable': 443,\n",
       " 'may': 444,\n",
       " 'were': 445,\n",
       " 'manuals': 446,\n",
       " 'box': 447,\n",
       " 'showing': 448,\n",
       " 'properly': 449,\n",
       " 'tablets': 450,\n",
       " 'secure': 451,\n",
       " 'dental': 452,\n",
       " 'adhesive': 453,\n",
       " 'father': 454,\n",
       " 'he': 455,\n",
       " 'satisfied': 456,\n",
       " 'decent': 457,\n",
       " 'shown': 458,\n",
       " 'photos': 459,\n",
       " 'thank': 460,\n",
       " 'amazon': 461,\n",
       " 'saved': 462,\n",
       " 'paint': 463,\n",
       " 'type': 464,\n",
       " 'costly': 465,\n",
       " 'digitally': 466,\n",
       " 'print': 467,\n",
       " 'Home': 468,\n",
       " 'find': 469,\n",
       " 'older': 470,\n",
       " 'harder': 471,\n",
       " 'lose': 472,\n",
       " 'weight': 473,\n",
       " '....': 474,\n",
       " 'gotten': 475,\n",
       " 'taking': 476,\n",
       " 'garcinia': 477,\n",
       " 'month': 478,\n",
       " 'take': 479,\n",
       " 'before': 480,\n",
       " 'breakfast': 481,\n",
       " 'lunch': 482,\n",
       " 'supper': 483,\n",
       " 'after': 484,\n",
       " 'tend': 485,\n",
       " 'eat': 486,\n",
       " 'late': 487,\n",
       " 'evenings': 488,\n",
       " 'cuts': 489,\n",
       " 'down': 490,\n",
       " 'appetite': 491,\n",
       " 'lost': 492,\n",
       " '8': 493,\n",
       " 'lbs': 494,\n",
       " 'which': 495,\n",
       " 'long': 496,\n",
       " 'weeks': 497,\n",
       " 'pictures': 498,\n",
       " '!!': 499,\n",
       " 'recommended': 500,\n",
       " 'buy': 501,\n",
       " 'suggested': 502,\n",
       " 'dosage': 503,\n",
       " 'felt': 504,\n",
       " 'extra': 505,\n",
       " 'energy': 506,\n",
       " 'loss': 507,\n",
       " 'pounds': 508,\n",
       " 'actual': 509,\n",
       " 'power': 510,\n",
       " 'supply': 511,\n",
       " 'cable': 512,\n",
       " 'harness': 513,\n",
       " 'completely': 514,\n",
       " 'taken': 515,\n",
       " 'apart': 516,\n",
       " 're': 517,\n",
       " 'oriented': 518,\n",
       " 'avoid': 519,\n",
       " 'birds': 520,\n",
       " 'nest': 521,\n",
       " 'backwards': 522,\n",
       " 'wiring': 523,\n",
       " 'tower': 524,\n",
       " 'test': 525,\n",
       " 'see': 526,\n",
       " 'personal': 527,\n",
       " 'physician': 528,\n",
       " 'goo': 529,\n",
       " 'gl': 530,\n",
       " 'wa7ai': 531,\n",
       " '$': 532,\n",
       " '10': 533,\n",
       " 'off': 534,\n",
       " 'free': 535,\n",
       " 'samples': 536,\n",
       " '!!!!!<': 537,\n",
       " 'told': 538,\n",
       " 'enormous': 539,\n",
       " 'benifit': 540,\n",
       " 'oregano': 541,\n",
       " 'powerful': 542,\n",
       " 'healing': 543,\n",
       " 'herbs': 544,\n",
       " 'natural': 545,\n",
       " 'anti': 546,\n",
       " 'biotics': 547,\n",
       " 'ever': 548,\n",
       " 'studied': 549,\n",
       " 'found': 550,\n",
       " 'study': 551,\n",
       " 'significantly': 552,\n",
       " '18': 553,\n",
       " 'currently': 554,\n",
       " 'treatment': 555,\n",
       " 'mrsa': 556,\n",
       " 'staph': 557,\n",
       " 'infections': 558,\n",
       " 'strong': 559,\n",
       " 'phenol': 560,\n",
       " 'oxidants': 561,\n",
       " 'destroy': 562,\n",
       " 'pathogenic': 563,\n",
       " 'bacteria': 564,\n",
       " 'viruses': 565,\n",
       " 'yeasts': 566,\n",
       " 'super': 567,\n",
       " 'herb': 568,\n",
       " 'rich': 569,\n",
       " 'oxidant': 570,\n",
       " 'phytochemical': 571,\n",
       " 'flavonoids': 572,\n",
       " 'phenolic': 573,\n",
       " 'acids': 574,\n",
       " 'third': 575,\n",
       " 'highest': 576,\n",
       " 'oxygen': 577,\n",
       " 'radical': 578,\n",
       " 'absorbancy': 579,\n",
       " 'capacity': 580,\n",
       " 'orac': 581,\n",
       " ')': 582,\n",
       " 'impressive': 583,\n",
       " 'score': 584,\n",
       " '200': 585,\n",
       " '129': 586,\n",
       " 'usda': 587,\n",
       " 'ranks': 588,\n",
       " 'antioxidant': 589,\n",
       " 'anywhere': 590,\n",
       " 'from': 591,\n",
       " '20': 592,\n",
       " 'higher': 593,\n",
       " 'can': 594,\n",
       " 'bottles': 595,\n",
       " 'oreganol': 596,\n",
       " 'p73': 597,\n",
       " 'fridge': 598,\n",
       " 'wowww': 599,\n",
       " 'miraculously': 600,\n",
       " 'discomfort': 601,\n",
       " 'dryness': 602,\n",
       " 'throat': 603,\n",
       " 'then': 604,\n",
       " 'couple': 605,\n",
       " 'drops': 606,\n",
       " 'tablespoon': 607,\n",
       " 'water': 608,\n",
       " 'drank': 609,\n",
       " 'healed': 610,\n",
       " 'soothed': 611,\n",
       " 'instantly': 612,\n",
       " 'feel': 613,\n",
       " 'another': 614,\n",
       " 'body': 615,\n",
       " 'aching': 616,\n",
       " 'set': 617,\n",
       " 'seconds': 618,\n",
       " 'fantastic': 619,\n",
       " '!!!': 620,\n",
       " 'emergency': 621,\n",
       " 'medicine': 622,\n",
       " 'eliminate': 623,\n",
       " 'digestive': 624,\n",
       " 'distress': 625,\n",
       " 'eating': 626,\n",
       " 'contaminated': 627,\n",
       " 'food': 628,\n",
       " 'fact': 629,\n",
       " 'easily': 630,\n",
       " 'read': 631,\n",
       " 'measurements': 632,\n",
       " 'those': 633,\n",
       " 'who': 634,\n",
       " 'eyes': 635,\n",
       " 'aren': 636,\n",
       " 'lol': 637,\n",
       " 'tired': 638,\n",
       " 'numbers': 639,\n",
       " 'others': 640,\n",
       " 'cup': 641,\n",
       " 'itself': 642,\n",
       " 'several': 643,\n",
       " 'seller': 644,\n",
       " 'green': 645,\n",
       " 'coffee': 646,\n",
       " 'exactly': 647,\n",
       " 'says': 648,\n",
       " 'am': 649,\n",
       " 'purchase': 650,\n",
       " 'finished': 651,\n",
       " 'bottle': 652,\n",
       " 'extremely': 653,\n",
       " 'electric': 654,\n",
       " 'blanket': 655,\n",
       " 'we': 656,\n",
       " '&': 657,\n",
       " 'both': 658,\n",
       " 'cannot': 659,\n",
       " 'wires': 660,\n",
       " 'material': 661,\n",
       " 'luxurious': 662,\n",
       " 'quite': 663,\n",
       " 'pricey': 664,\n",
       " '50': 665,\n",
       " 'ones': 666,\n",
       " 'being': 667,\n",
       " 'indicates': 668,\n",
       " 'lover': 669,\n",
       " 'want': 670,\n",
       " 'flavor': 671,\n",
       " 'clean': 672,\n",
       " 'figured': 673,\n",
       " 'fit': 674,\n",
       " 'dasani': 675,\n",
       " 'able': 676,\n",
       " 'dinnerware': 677,\n",
       " 'aesthetic': 678,\n",
       " 'allow': 679,\n",
       " 'decoratively': 680,\n",
       " 'beauty': 681,\n",
       " 'white': 682,\n",
       " 'goes': 683,\n",
       " 'scheme': 684,\n",
       " 'acquiring': 685,\n",
       " 'already': 686,\n",
       " 'owned': 687,\n",
       " 'plain': 688,\n",
       " ';': 689,\n",
       " 'platinum': 690,\n",
       " 'rim': 691,\n",
       " 'nicer': 692,\n",
       " 'sides': 693,\n",
       " 'match': 694,\n",
       " 'tan': 695,\n",
       " 'became': 696,\n",
       " 'blackish': 697,\n",
       " 'parts': 698,\n",
       " 'unpleasant': 699,\n",
       " 'since': 700,\n",
       " 'pain': 701,\n",
       " 'tolerance': 702,\n",
       " 'facial': 703,\n",
       " 'hair': 704,\n",
       " 'remover': 705,\n",
       " 'hurt': 706,\n",
       " 'face': 707,\n",
       " 'hate': 708,\n",
       " 'peach': 709,\n",
       " 'fuzz': 710,\n",
       " 'wearing': 711,\n",
       " 'foundation': 712,\n",
       " 'tweezers': 713,\n",
       " 'okay': 714,\n",
       " 'loved': 715,\n",
       " 'hard': 716,\n",
       " 'package': 717,\n",
       " 'bubble': 718,\n",
       " 'wrapped': 719,\n",
       " 'cute': 720,\n",
       " 'mine': 721,\n",
       " 'got': 722,\n",
       " 'smashed': 723,\n",
       " 'process': 724,\n",
       " 'shipping': 725,\n",
       " 'customer': 726,\n",
       " 'service': 727,\n",
       " 'bag': 728,\n",
       " 'smaller': 729,\n",
       " 'version': 730,\n",
       " 'picture': 731,\n",
       " 'shows': 732,\n",
       " 'describes': 733,\n",
       " 'holds': 734,\n",
       " 'accommodate': 735,\n",
       " 'd3100': 736,\n",
       " 'lenses': 737,\n",
       " 'along': 738,\n",
       " 'charger': 739,\n",
       " 'gel': 740,\n",
       " 'eyeliner': 741,\n",
       " 'longer': 742,\n",
       " 'liked': 743,\n",
       " 'slanted': 744,\n",
       " 'apply': 745,\n",
       " 'mask': 746,\n",
       " 'purpose': 747,\n",
       " 'keeping': 748,\n",
       " 'light': 749,\n",
       " 'hubs': 750,\n",
       " 'gets': 751,\n",
       " '4am': 752,\n",
       " 'butthead': 753,\n",
       " 'turning': 754,\n",
       " 'lights': 755,\n",
       " 'getting': 756,\n",
       " 'ive': 757,\n",
       " 'havent': 758,\n",
       " 'issues': 759,\n",
       " 'sleeping': 760,\n",
       " 'through': 761,\n",
       " 'mini': 762,\n",
       " 'bra': 763,\n",
       " 'traditional': 764,\n",
       " 'flat': 765,\n",
       " 'sleep': 766,\n",
       " 'masks': 767,\n",
       " 'lay': 768,\n",
       " '&#': 769,\n",
       " '34': 770,\n",
       " 'cups': 771,\n",
       " 'touch': 772,\n",
       " 'eyelids': 773,\n",
       " 'itchy': 774,\n",
       " 'reason': 775,\n",
       " 'why': 776,\n",
       " 'didnt': 777,\n",
       " 'give': 778,\n",
       " 'whenever': 779,\n",
       " 'turned': 780,\n",
       " 'kinda': 781,\n",
       " 'uncomfortable': 782,\n",
       " 'make': 783,\n",
       " 'stay': 784,\n",
       " ':(': 785,\n",
       " 'photoshoot': 786,\n",
       " '..': 787,\n",
       " 'cool': 788,\n",
       " 'vintage': 789,\n",
       " 'awesome': 790,\n",
       " 'Jewelry': 791,\n",
       " '5zi8i': 792,\n",
       " '600mg': 793,\n",
       " 'tablet': 794,\n",
       " 'cents': 795,\n",
       " 'additional': 796,\n",
       " 'biochemical': 797,\n",
       " 'pharmacological': 798,\n",
       " 'studies': 799,\n",
       " 'identified': 800,\n",
       " 'red': 801,\n",
       " 'yeast': 802,\n",
       " 'rice': 803,\n",
       " 'beneficial': 804,\n",
       " 'maintaining': 805,\n",
       " 'healthy': 806,\n",
       " 'balance': 807,\n",
       " 'cholesterol': 808,\n",
       " 'related': 809,\n",
       " 'lipids': 810,\n",
       " 'reported': 811,\n",
       " 'lowering': 812,\n",
       " 'ldl': 813,\n",
       " 'triglycerides': 814,\n",
       " 'while': 815,\n",
       " 'increasing': 816,\n",
       " 'hdl': 817,\n",
       " 'clinical': 818,\n",
       " 'patients': 819,\n",
       " 'suffered': 820,\n",
       " 'heart': 821,\n",
       " 'attack': 822,\n",
       " 'partially': 823,\n",
       " 'purified': 824,\n",
       " 'extract': 825,\n",
       " 'reduced': 826,\n",
       " 'risk': 827,\n",
       " 'repeat': 828,\n",
       " '45': 829,\n",
       " 'revascularization': 830,\n",
       " 'bypass': 831,\n",
       " 'surgery': 832,\n",
       " 'angioplasty': 833,\n",
       " 'cardiovascular': 834,\n",
       " 'mortality': 835,\n",
       " 'total': 836,\n",
       " 'cancer': 837,\n",
       " 'thirds': 838,\n",
       " 'nattokinase': 839,\n",
       " 'combined': 840,\n",
       " 'lower': 841,\n",
       " '32eeb': 842,\n",
       " 'fabulous': 843,\n",
       " 'job': 844,\n",
       " 'reducing': 845,\n",
       " 'overall': 846,\n",
       " '220': 847,\n",
       " '150': 848,\n",
       " ').': 849,\n",
       " 'amazing': 850,\n",
       " 'reviews': 851,\n",
       " 'fake': 852,\n",
       " 'buyer': 853,\n",
       " 'ware': 854,\n",
       " 'blood': 855,\n",
       " 'stool': 856,\n",
       " 'stopped': 857,\n",
       " 'right': 858,\n",
       " 'away': 859,\n",
       " 'positive': 860,\n",
       " 'thing': 861,\n",
       " 'reviewer': 862,\n",
       " 'anyone': 863,\n",
       " 'write': 864,\n",
       " '1': 865,\n",
       " 'star': 866,\n",
       " 'review': 867,\n",
       " '5hndv': 868,\n",
       " 'biotin': 869,\n",
       " 'role': 870,\n",
       " 'multi': 871,\n",
       " 'faceted': 872,\n",
       " 'number': 873,\n",
       " 'nails': 874,\n",
       " 'help': 875,\n",
       " 'grow': 876,\n",
       " 'treat': 877,\n",
       " 'brittle': 878,\n",
       " 'toe': 879,\n",
       " 'fingernails': 880,\n",
       " 'alopecia': 881,\n",
       " 'people': 882,\n",
       " 'premature': 883,\n",
       " 'gray': 884,\n",
       " 'levels': 885,\n",
       " 'helped': 886,\n",
       " 'combat': 887,\n",
       " 'graying': 888,\n",
       " 'candida': 889,\n",
       " 'changing': 890,\n",
       " 'fungal': 891,\n",
       " 'form': 892,\n",
       " 'fat': 893,\n",
       " 'burner': 894,\n",
       " 'often': 895,\n",
       " 'promoted': 896,\n",
       " '\"': 897,\n",
       " 'principle': 898,\n",
       " 'roles': 899,\n",
       " 'metabolism': 900,\n",
       " 'fats': 901,\n",
       " 'proteins': 902,\n",
       " 'carbs': 903,\n",
       " '4': 904,\n",
       " 'reduce': 905,\n",
       " 'sugar': 906,\n",
       " 'cases': 907,\n",
       " 'doctors': 908,\n",
       " 'diabetes': 909,\n",
       " 'gene': 910,\n",
       " 'replication': 911,\n",
       " 'deeper': 912,\n",
       " 'level': 913,\n",
       " 'integral': 914,\n",
       " 'cells': 915,\n",
       " 'bone': 916,\n",
       " 'marrow': 917,\n",
       " 'production': 918,\n",
       " 'thinning': 919,\n",
       " 'working': 920,\n",
       " 'stressful': 921,\n",
       " 'point': 922,\n",
       " 'couldn': 923,\n",
       " 'successfully': 924,\n",
       " 'ponytail': 925,\n",
       " 'decided': 926,\n",
       " 'try': 927,\n",
       " 'report': 928,\n",
       " 'stronger': 929,\n",
       " 'thicker': 930,\n",
       " 'haven': 931,\n",
       " 'seen': 932,\n",
       " 'growth': 933,\n",
       " 'length': 934,\n",
       " 'prettier': 935,\n",
       " 'manageable': 936,\n",
       " 'hairdresser': 937,\n",
       " 'surprised': 938,\n",
       " 'went': 939,\n",
       " 'her': 940,\n",
       " 'trim': 941,\n",
       " 'haircut': 942,\n",
       " 'normally': 943,\n",
       " 'reach': 944,\n",
       " ':)': 945,\n",
       " 'calcium': 946,\n",
       " 'hampers': 947,\n",
       " 'burning': 948,\n",
       " 'october': 949,\n",
       " 'losing': 950,\n",
       " 'dont': 951,\n",
       " 'care': 952,\n",
       " 'contains': 953,\n",
       " 'going': 954,\n",
       " 'worked': 955,\n",
       " 'expected': 956,\n",
       " 'lightweight': 957,\n",
       " 'durable': 958,\n",
       " 'perfect': 959,\n",
       " 'wife': 960,\n",
       " 'although': 961,\n",
       " 'still': 962,\n",
       " 'spare': 963,\n",
       " 'stylus': 964,\n",
       " 'pens': 965,\n",
       " 'know': 966,\n",
       " 'xv1lp': 967,\n",
       " 'pine': 968,\n",
       " 'bark': 969,\n",
       " 'treating': 970,\n",
       " 'diseases': 971,\n",
       " 'associated': 972,\n",
       " 'effects': 973,\n",
       " 'radicals': 974,\n",
       " 'prevention': 975,\n",
       " 'against': 976,\n",
       " ':<': 977,\n",
       " 'asthma': 978,\n",
       " '<': 979,\n",
       " 'chronic': 980,\n",
       " 'venous': 981,\n",
       " 'insufficiency': 982,\n",
       " 'condition': 983,\n",
       " 'involves': 984,\n",
       " 'leg': 985,\n",
       " 'swelling': 986,\n",
       " 'varicose': 987,\n",
       " 'veins': 988,\n",
       " 'itching': 989,\n",
       " 'changes': 990,\n",
       " ')<': 991,\n",
       " 'pressure': 992,\n",
       " 'hypertension': 993,\n",
       " 'retinopathy': 994,\n",
       " 'eye': 995,\n",
       " 'commonly': 996,\n",
       " 'caused': 997,\n",
       " 'attention': 998,\n",
       " 'deficit': 999,\n",
       " 'hyperactivity': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-fold number: 1\n",
      "Testing data length: 1680\n",
      "Training data length: 15120\n",
      "Training Classifier...\n",
      "K-fold number: 2\n",
      "Testing data length: 1680\n",
      "Training data length: 15120\n",
      "Training Classifier...\n",
      "K-fold number: 3\n",
      "Testing data length: 1680\n",
      "Training data length: 15120\n",
      "Training Classifier...\n",
      "K-fold number: 4\n",
      "Testing data length: 1680\n",
      "Training data length: 15120\n",
      "Training Classifier...\n",
      "K-fold number: 5\n",
      "Testing data length: 1680\n",
      "Training data length: 15120\n",
      "Training Classifier...\n",
      "K-fold number: 6\n",
      "Testing data length: 1680\n",
      "Training data length: 15120\n",
      "Training Classifier...\n",
      "K-fold number: 7\n",
      "Testing data length: 1680\n",
      "Training data length: 15120\n",
      "Training Classifier...\n",
      "K-fold number: 8\n",
      "Testing data length: 1680\n",
      "Training data length: 15120\n",
      "Training Classifier...\n",
      "K-fold number: 9\n",
      "Testing data length: 1680\n",
      "Training data length: 15120\n",
      "Training Classifier...\n",
      "K-fold number: 10\n",
      "Testing data length: 1680\n",
      "Training data length: 15120\n",
      "Training Classifier...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7901154039425918,\n",
       " 0.7886904761904762,\n",
       " 0.7884678175004826,\n",
       " 0.7886904761904762]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# QUESTION 3 - Make sure there is a function call here to the\n",
    "# crossValidate function on the training set to get your results\n",
    "crossValidate(trainData, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({7: 0.08, 4068: 0.04, 36: 0.04, 216: 0.04, 18655: 0.04, 42: 0.04, 318: 0.04, 52: 0.04, 66: 0.04, 51: 0.04, 13: 0.08, 11: 0.08, 128: 0.04, 666: 0.04, 28: 0.04, 1510: 0.04, 1213: 0.04, 779: 0.04, 1704: 0.04, 1693: 0.04, 152: 0.04, 102: 0.04, 23: 1.1, 24: 5, 1582: 1}, 'fake')\n",
      "Training Classifier...\n",
      "\n",
      "Done training!\n",
      "Precision: 0.802117\n",
      "Recall: 0.798333\n",
      "F Score:0.797700\n",
      "Accuracy: 0.798333\n"
     ]
    }
   ],
   "source": [
    "# Finally, check the accuracy of your classifier by training on all the tranin data\n",
    "# and testing on the test set\n",
    "# Will only work once all functions are complete\n",
    "functions_complete = True  # set to True once you're happy with your methods for cross val\n",
    "if functions_complete:\n",
    "    print(testData[0])   # have a look at the first test data instance\n",
    "    classifier = trainClassifier(trainData)  # train the classifier\n",
    "    testTrue = [t[1] for t in testData]   # get the ground-truth labels from the data\n",
    "    testPred = predictLabels(testData, classifier)  # classify the test data to get predicted labels\n",
    "    finalScores = precision_recall_fscore_support(testTrue, testPred, average='weighted') # evaluate\n",
    "    accuracy  = metrics.accuracy_score(testTrue, testPred)\n",
    "    print(\"\\nDone training!\")\n",
    "    print(\"Precision: %f\\nRecall: %f\\nF Score:%f\" % finalScores[:3])\n",
    "    print(\"Accuracy: %0.6f\" % accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effects of adding new features\n",
    "Adding the new features made a substantial difference to the success of the algorithm. The results of the different combinations of the features are given below. The inclusion of the verified purchase feature makes the most signifcant difference, increasing the accuracy from 64% to 80+%. The feature combination that produces the best results are purchase verification and rating. The category feature lowers the algorithm effectiveness in all combinations.\n",
    "\n",
    "There was an increase of roughly 15% in the recall, precision, F score and accuracy of the results compared with the improved classifer from question 4. These fe\n",
    "\n",
    "\n",
    "VERIFIED PURCHASE, RATING & CATEGORY:\n",
    "\n",
    "    Precision: 0.802117\n",
    "    Recall: 0.798333\n",
    "    F Score:0.797700\n",
    "    Accuracy: 0.798333\n",
    "\n",
    "VERIFIED PURCHASE & RATING:\n",
    "\n",
    "    Precision: 0.813008\n",
    "    Recall: 0.809286\n",
    "    F Score:0.808717\n",
    "    Accuracy: 0.809286\n",
    "    \n",
    "VERIFIED PURCHASE & CATEGORY:\n",
    "\n",
    "    Precision: 0.806133\n",
    "    Recall: 0.802619\n",
    "    F Score:0.802051\n",
    "    Accuracy: 0.802619\n",
    "    \n",
    "RATING & CATEGORY:\n",
    "\n",
    "    Precision: 0.483024\n",
    "    Recall: 0.483571\n",
    "    F Score:0.479377\n",
    "    Accuracy: 0.483571\n",
    "    \n",
    "VERIFIED PURCHASE ONLY:\n",
    "\n",
    "    Precision: 0.813151\n",
    "    Recall: 0.809524\n",
    "    F Score:0.808971\n",
    "    Accuracy: 0.809524\n",
    "\n",
    "RATING ONLY:\n",
    "\n",
    "    Precision: 0.591861\n",
    "    Recall: 0.591429\n",
    "    F Score:0.590948\n",
    "    Accuracy: 0.591429\n",
    "    \n",
    "CATEGORY ONLY:\n",
    "\n",
    "    Precision: 0.522724\n",
    "    Recall: 0.522381\n",
    "    F Score:0.520572\n",
    "    Accuracy: 0.522381\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([0.08, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.08, 0.08, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 1.1, 5, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [t[1] for t in testData]\n",
    "vp = testData[0]\n",
    "vp[0].values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fake'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vp[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2100"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake = 0\n",
    "fake_vps = 0\n",
    "for i in range(len(testData)):\n",
    "    # Number of fake\n",
    "    if testData[i][1] == \"fake\":\n",
    "        fake +=1\n",
    "fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
