{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv                               # csv reader\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.classify import SklearnClassifier\n",
    "from random import shuffle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from a file and append it to the rawData\n",
    "def loadData(path, Text=None):\n",
    "    with open(path) as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        for line in reader:\n",
    "            if line[0] == \"DOC_ID\":  # skip the header\n",
    "                continue\n",
    "            (Id, Text, Label) = parseReview(line)\n",
    "            rawData.append((Id, Text, Label))\n",
    "\n",
    "def splitData(percentage):\n",
    "    # A method to split the data between trainData and testData \n",
    "    dataSamples = len(rawData)\n",
    "    halfOfData = int(len(rawData)/2)\n",
    "    trainingSamples = int((percentage*dataSamples)/2)\n",
    "    for (_, Text, Label) in rawData[:trainingSamples] + rawData[halfOfData:halfOfData+trainingSamples]:\n",
    "        trainData.append((toFeatureVector(preProcess(Text)),Label))\n",
    "    for (_, Text, Label) in rawData[trainingSamples:halfOfData] + rawData[halfOfData+trainingSamples:]:\n",
    "        testData.append((toFeatureVector(preProcess(Text)),Label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert line from input file into an id/text/label tuple\n",
    "def parseReview(reviewLine):\n",
    "    # Should return a triple of an integer, a string containing the review, and a string indicating the label\n",
    "    \"\"\"The ID, text and label are found by slicing each line. The ID is \n",
    "    the first element and the label is the second element. The text is\n",
    "    the last element\"\"\"\n",
    "    \n",
    "    doc_id = reviewLine[0]\n",
    "    review_text = reviewLine[-1]\n",
    "    label = reviewLine[1]\n",
    "    if label == \"__label1__\":\n",
    "        label = \"fake\"\n",
    "    else:\n",
    "        label= \"real\"\n",
    "    \n",
    "    return (doc_id, review_text, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEXT PREPROCESSING AND FEATURE VECTORIZATION\n",
    "\n",
    "# Input: a string of one review\n",
    "def preProcess(text):\n",
    "    \"\"\"Each line of text is preprocessed into tokens by splitting on \n",
    "    non-alphanumeric characters. The whitespace tokens are then removed.\n",
    "    The text is then normalised by making all tokens lowercase.\"\"\"\n",
    "    token_split = re.split(r'[\\W]', text)\n",
    "    token_list = []\n",
    "    for token in token_split:\n",
    "        if token != '':\n",
    "            token_list.append(token)\n",
    "    token_list = [token.lower() for token in token_list]\n",
    "    return token_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureDict = {} # A global dictionary of features\n",
    "\n",
    "def toFeatureVector(tokens):\n",
    "    \"\"\"A feature vector of the tokens is then created. For each instance\n",
    "    a local dictionary is created with features as keys and weights the \n",
    "    number of occurrences of the token. A global feature dictionary is also\n",
    "    created that contains all features found in the local dictionaries.\"\"\"\n",
    "    \n",
    "    localDict = {}\n",
    "    \n",
    "    \"\"\"In each token list the token is first assumed to be within the dictionary,\n",
    "    and increases the weight by an increment of 1. This is done using the \"try\"\n",
    "    clause. If there is no pre-existing feature within the dictonary, \n",
    "    an exception occurs and a feature is created with corresponding value\n",
    "    of 1.\"\"\"\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            i = featureDict[token]\n",
    "        except KeyError:\n",
    "            i = len(featureDict) + 1\n",
    "            featureDict[token] = i\n",
    "        try:\n",
    "            localDict[i] += 1\n",
    "        except KeyError:\n",
    "            localDict[i] =1\n",
    "    return localDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING AND VALIDATING OUR CLASSIFIER\n",
    "def trainClassifier(trainData):\n",
    "    print(\"Training Classifier...\")\n",
    "    pipeline =  Pipeline([('svc', LinearSVC(dual=True, max_iter=20000))])\n",
    "    return SklearnClassifier(pipeline).train(trainData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def crossValidate(dataset, folds):\n",
    "    \"\"\"Cross validate splits the dataset into a number of folds, which \n",
    "    resamples the data to evaluate the model of the classifier. The data is \n",
    "    split into training and testing data by a number of folds, which then validate\n",
    "    the model and assess different characteristics. The crossValidate function\n",
    "    gives the averaged precision, recall, f-score and accuracy of the model.\"\"\"\n",
    "\n",
    "    # The data is first shuffled\n",
    "    shuffle(dataset)\n",
    "    \n",
    "    # Initial values of each characteristic are set to 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    f_score = 0\n",
    "    accuracy = 0\n",
    "    k_num = 1\n",
    "    # The size of the test folds\n",
    "    foldSize = int(len(dataset)/folds)\n",
    "    \n",
    "    # The loop of the cross-validate.\n",
    "    # The dataset is split into testing and training data sets\n",
    "    \n",
    "    for i in range(0,len(dataset),foldSize):\n",
    "        \"\"\"Splits the data into testing and training sets\"\"\"\n",
    "        \n",
    "        testing_data = dataset[i:i+foldSize]\n",
    "        print(f'Testing data length: {len(testing_data)}')\n",
    "        training_data = dataset[0:i] + dataset[(i+foldSize):]\n",
    "        print(f'Training data length: {len(training_data)}')\n",
    "        \n",
    "        print(f'K-fold number: {k_num}')\n",
    "        k_num +=1\n",
    "        \n",
    "        # Create classifier using each training set \n",
    "        # The true labels for the corresponding testing data are found\n",
    "        classifier = trainClassifier(training_data)\n",
    "        true_labels = [t[1] for t in testing_data]\n",
    "\n",
    "        # Prediction on the unseen test data using the classifer\n",
    "        test_pred = predictLabels(testing_data, classifier)\n",
    "        \n",
    "        # The precision, recall and f-score of the results\n",
    "        (p, r, f, s) = metrics.precision_recall_fscore_support(true_labels, test_pred, average='weighted')\n",
    "        \n",
    "        # The values found in each loop are totalled\n",
    "        precision += p\n",
    "        recall += r\n",
    "        f_score += f\n",
    "        \n",
    "        # The accuracy of the classifer is found and summed\n",
    "        a = metrics.accuracy_score(true_labels, test_pred)\n",
    "        accuracy += a\n",
    "#         continue # Replace by code that trains and tests on the 10 folds of data in the dataset\n",
    "    \n",
    "    # The mean of the characteristics is found and returned.\n",
    "    precision /= folds\n",
    "    recall /= folds \n",
    "    f_score /= folds\n",
    "    accuracy /= folds\n",
    "    cv_results = [precision, recall, f_score, accuracy]\n",
    "    \n",
    "    return cv_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTING LABELS GIVEN A CLASSIFIER\n",
    "# Use predict labels\n",
    "def predictLabels(reviewSamples, classifier):\n",
    "    return classifier.classify_many(map(lambda t: t[0], reviewSamples))\n",
    "\n",
    "def predictLabel(reviewSample, classifier):\n",
    "    return classifier.classify(toFeatureVector(preProcess(reviewSample)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now 0 rawData, 0 trainData, 0 testData\n",
      "Preparing the dataset...\n",
      "Now 21000 rawData, 0 trainData, 0 testData\n",
      "Preparing training and test data...\n",
      "After split, 21000 rawData, 16800 trainData, 4200 testData\n",
      "Training Samples: \n",
      "16800\n",
      "Features: \n",
      "34913\n"
     ]
    }
   ],
   "source": [
    "# MAIN\n",
    "\n",
    "# loading reviews\n",
    "# initialize global lists that will be appended to by the methods below\n",
    "rawData = []          # the filtered data from the dataset file (should be 21000 samples)\n",
    "trainData = []        # the pre-processed training data as a percentage of the total dataset (currently 80%, or 16800 samples)\n",
    "testData = []         # the pre-processed test data as a percentage of the total dataset (currently 20%, or 4200 samples)\n",
    "\n",
    "# the output classes\n",
    "fakeLabel = 'fake'\n",
    "realLabel = 'real'\n",
    "\n",
    "# references to the data files\n",
    "reviewPath = 'amazon_reviews.txt'\n",
    "\n",
    "# Do the actual stuff (i.e. call the functions we've made)\n",
    "# We parse the dataset and put it in a raw data list\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Preparing the dataset...\",sep='\\n')\n",
    "loadData(reviewPath) \n",
    "\n",
    "# We split the raw dataset into a set of training data and a set of test data (80/20)\n",
    "# You do the cross validation on the 80% (training data)\n",
    "# We print the number of training samples and the number of features before the split\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Preparing training and test data...\",sep='\\n')\n",
    "splitData(0.8)\n",
    "# We print the number of training samples and the number of features after the split\n",
    "print(\"After split, %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Training Samples: \", len(trainData), \"Features: \", len(featureDict), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data length: 1680\n",
      "Training data length: 15120\n",
      "K-fold number: 1\n",
      "Training Classifier...\n",
      "Testing data length: 1680\n",
      "Training data length: 15120\n",
      "K-fold number: 2\n",
      "Training Classifier...\n",
      "Testing data length: 1680\n",
      "Training data length: 15120\n",
      "K-fold number: 3\n",
      "Training Classifier...\n",
      "Testing data length: 1680\n",
      "Training data length: 15120\n",
      "K-fold number: 4\n",
      "Training Classifier...\n",
      "Testing data length: 1680\n",
      "Training data length: 15120\n",
      "K-fold number: 5\n",
      "Training Classifier...\n",
      "Testing data length: 1680\n",
      "Training data length: 15120\n",
      "K-fold number: 6\n",
      "Training Classifier...\n",
      "Testing data length: 1680\n",
      "Training data length: 15120\n",
      "K-fold number: 7\n",
      "Training Classifier...\n",
      "Testing data length: 1680\n",
      "Training data length: 15120\n",
      "K-fold number: 8\n",
      "Training Classifier...\n",
      "Testing data length: 1680\n",
      "Training data length: 15120\n",
      "K-fold number: 9\n",
      "Training Classifier...\n",
      "Testing data length: 1680\n",
      "Training data length: 15120\n",
      "K-fold number: 10\n",
      "Training Classifier...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6132936793808013,\n",
       " 0.6127976190476191,\n",
       " 0.6127133664427085,\n",
       " 0.6127976190476191]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# QUESTION 3 - Make sure there is a function call here to the\n",
    "# crossValidate function on the training set to get your results\n",
    "crossValidate(trainData, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({6: 2, 3952: 1, 31: 1, 201: 1, 18276: 1, 300: 1, 46: 1, 58: 1, 45: 1, 10: 2, 118: 1, 628: 1, 23: 1, 1447: 1, 1157: 1, 739: 1, 1634: 1, 1623: 1, 141: 1, 93: 1}, 'fake')\n",
      "Training Classifier...\n",
      "\n",
      "Done training!\n",
      "Precision: 0.592891\n",
      "Recall: 0.592857\n",
      "F Score:0.592820\n",
      "Accuracy: 0.592857\n"
     ]
    }
   ],
   "source": [
    "# Finally, check the accuracy of your classifier by training on all the tranin data\n",
    "# and testing on the test set\n",
    "# Will only work once all functions are complete\n",
    "functions_complete = True  # set to True once you're happy with your methods for cross val\n",
    "if functions_complete:\n",
    "    print(testData[0])   # have a look at the first test data instance\n",
    "    classifier = trainClassifier(trainData)  # train the classifier\n",
    "    testTrue = [t[1] for t in testData]   # get the ground-truth labels from the data\n",
    "    testPred = predictLabels(testData, classifier)  #Â classify the test data to get predicted labels\n",
    "    finalScores = precision_recall_fscore_support(testTrue, testPred, average='weighted') # evaluate\n",
    "    accuracy  = metrics.accuracy_score(testTrue, testPred)\n",
    "    print(\"\\nDone training!\")\n",
    "    print(\"Precision: %f\\nRecall: %f\\nF Score:%f\" % finalScores[:3])\n",
    "    print(\"Accuracy: %0.6f\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions 4 and 5\n",
    "Once you're happy with your functions for Questions 1 to 3, it's advisable you make a copy of this notebook to make a new notebook, and then within it adapt and improve all three functions in the ways asked for in questions 4 and 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
